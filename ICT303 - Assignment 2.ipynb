{"cells":[{"cell_type":"markdown","metadata":{"id":"7mKK4Rd8vhPS"},"source":["#  **ICT303 - Assignment 2**\n","\n","**Your name: <enter here your full name>**\n","\n","**Student ID: <enter here your student ID>** \n","\n","**Email: <enter here your email address>** \n","\n","In this assignment, you will build a deep learning model for identifying $120$ different breeds of dogs. Similar to the previous assignment, you will use real images from the [Kaggle competition](https://www.kaggle.com/c/dog-breed-identification). \n","\n","In this assignment, your are required to use a ResNet network. You can use ResNet implementation provided in PyTorch. Note however  that there are many versions of ResNet (they differ in terms of number of layers). Your task is to find the best configuration that gives the best performance. \n","\n","The rule is similar to the previous assignment:\n","\n","1. Develop a better model to reduce the recognition error.  \n","2. Submit your results to Kaggle and take a sceenshot of your score. Then insert here the screenshot of your result. \n","\n","It is important that you start as earlier as possible. Tuning hyper-parameters takes time, and Kaggle limits the number of submissions per day.\n","\n","The top 3 students in the Kaggle ranking will be invited for a coffee!"]},{"cell_type":"markdown","metadata":{"id":"QYa8reVO9OBQ"},"source":["## **1. Obtaining and Organizing the Data Set**\n","\n","The competition data is divided into a training set and testing set:\n","- The training set contains $10,222$ color images.\n","- The testing set contains $10,357$ color images. \n","\n","The images in both sets are in JPEG format. Each image contains three channels (R, G and B). The images have  different heights and widths.\n","\n","There are $120$ breeds of dogs in the training set, e.g., *Labradors, Poodles, Dachshunds,\n","Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers*."]},{"cell_type":"markdown","metadata":{"id":"he01H0Em-AlP"},"source":["### **1.1. Downloading the Data Set**\n","\n","After logging in to Kaggle, click on the “Data” tab on the dog breed identification competition webpage and download:\n","- the training data set `train.zip` and their corresponing labels `label.csv.zip`,\n","- the testing data set `test.zip`, \n","\n","After downloading the files, place them in the three paths below:\n","- kaggle_dog/train.zip\n","- kaggle_dog/test.zip\n","- kaggle_dog/labels.csv.zip\n","\n","Run the code below to extract the data. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQl2ToXAuuIF"},"outputs":[],"source":["import zipfile\n","\n","data_dir = './kaggle_dog'\n","\n","zipfiles = ['train.zip', 'test.zip', 'labels.csv.zip']\n","for f in zipfiles:\n","  with zipfile.ZipFile(data_dir + '/' + f, 'r') as z:\n","    z.extractall(data_dir)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["data_dir = '../MLData-dog-breed-image-recognition'"]},{"cell_type":"markdown","metadata":{"id":"B6lWE7t9-Cfn"},"source":["### **1.2. Organizing the Data Set**\n","\n","Next, we define the reorg_train_valid function to split the validation set from the original Kaggle competition training set. The parameter valid_ratio in this function is the ratio of the number of examples of each dog breeds in the validation set to the number of examples of the\n","breed with the least examples (66) in the original training set. \n","\n","After organizing the data, images of the same breed will be placed in the same folder so that we can read them later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cl3UvrUBw70C"},"outputs":[],"source":["# Let's first install d2l package, since we will need some functions from this package\n","! pip install d2l==1.0.0a1.post0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! pip install torch\n","! pip install torchvision"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def mkdir_if_not_exist(path):\n","    if not isinstance(path, str):\n","        path = os.path.join(*path)\n","    os.makedirs(path, exist_ok=True)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"B5YtyOyyvrS2"},"outputs":[],"source":["import collections\n","import d2l\n","import shutil\n","import os\n","import math\n","\n","def reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label):\n","  # The number of examples of the least represented breed in the training set.\n","  min_n_train_per_label = (\n","      collections.Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n","  \n","  # The number of examples of each breed in the validation set.\n","  n_valid_per_label = math.floor(min_n_train_per_label * valid_ratio)\n","  label_count = {}\n","  for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n","    idx = train_file.split('.')[0]\n","    label = idx_label[idx]\n","\n","    mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n","    \n","    shutil.copy(os.path.join(data_dir, train_dir, train_file),\n","                os.path.join(data_dir, input_dir, 'train_valid', label))\n","    \n","    if label not in label_count or label_count[label] < n_valid_per_label:\n","      mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n","      shutil.copy(os.path.join(data_dir, train_dir, train_file),\n","                  os.path.join(data_dir, input_dir, 'valid', label))\n","      label_count[label] = label_count.get(label, 0) + 1\n","      \n","    else:\n","      mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n","      shutil.copy(os.path.join(data_dir, train_dir, train_file),\n","                  os.path.join(data_dir, input_dir, 'train', label))"]},{"cell_type":"markdown","metadata":{"id":"hO1gPYXCyX_t"},"source":["The `reorg_dog_data` function below is used to read the training data labels, segment the validation set, and organize the training set."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"gdsp7bSryXbQ"},"outputs":[],"source":["def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n","  # Read the training data labels.\n","  with open(os.path.join(data_dir, label_file), 'r') as f:\n","    # Skip the file header line (column name).\n","    lines = f.readlines()[1:]\n","    tokens = [l.rstrip().split(',') for l in lines]\n","    idx_label = dict(((idx, label) for idx, label in tokens))\n","  \n","  reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label)\n","\n","  # Organize the training set.\n","  mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n","  for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n","    shutil.copy(os.path.join(data_dir, test_dir, test_file),\n","                os.path.join(data_dir, input_dir, 'test', 'unknown'))"]},{"cell_type":"markdown","metadata":{"id":"trqywyYRysYi"},"source":["During actual training and testing, we would use the entire Kaggle Competition data set and call the reorg_dog_data function to organize the data set. Likewise, we would need to set the batch_size to a larger integer, such as 128."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"A3o_SZtRy4qt"},"outputs":[],"source":["label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'\n","input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n","reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"]},{"cell_type":"markdown","metadata":{"id":"Mh_N5wsV-FUv"},"source":["## **2. Image Augmentation**\n","\n","Sometimes, when we do not have enough images to train our deep learning model, we data augmentation to simulate new data. For example, in the case of images, assume we only have $10$ images per class. We can create more instance by applying transformations to these images. For example, if the image is of a standin dog, we can rotate it $90$ and $180$ degrees to create two additional instances of the same dog. We can also scale it, etc.\n","\n","Here are some more image augmentation operations that might be useful.\n","\n","Start by training your model on the data set, the way it is provided. Then, think of the types of transformations you can apply to the training images to improve the performance. \n","\n","You can find more about how to apply transformations to images in this [link](https://pytorch.org/vision/stable/transforms.html)."]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","\n","# Define the transformations to be applied to the images\n","transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load the dataset\n","train_valid_dataset = ImageFolder('../MLData-dog-breed-image-recognition/train_valid_test/train_valid/', transform=transform)\n","\n","# Create a dataloader to load the images in batches\n","train_valid_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Train your model using the augmented data"]},{"cell_type":"markdown","metadata":{"id":"QCUUbgO0-OcV"},"source":["## **3. Loading (Reading) the Data Set**"]},{"cell_type":"markdown","metadata":{"id":"HKE7gFVE1Heu"},"source":["Similar to previous labs, write here the Python code tat reads the training, validation and test set."]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# Define the transformations to be applied to the images\n","transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load the training set\n","train_dataset = ImageFolder('../MLData-dog-breed-image-recognition/train_valid_test/train/', transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Load the validation set\n","valid_dataset = ImageFolder('../MLData-dog-breed-image-recognition/train_valid_test/valid/', transform=transform)\n","valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n","\n","# Load the test set\n","test_dataset = ImageFolder('../MLData-dog-breed-image-recognition/train_valid_test/test/', transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"jeeuF3ul-Ucg"},"source":["## **4. Defining and Training ResNet**\n","\n","Here, you are required to use ResNet to recognise the breed of the dogs in the images. You need to write the class that defines the network, the training class and the code for training the network. You are not required to implement ResNet from scratch. Instead, use PyTorch's implementation of ResNet. \n","\n","Note that there are many versions of ResNet. They differ in the number of layers they use. You are required to test with at least 2 versions and report their respective performances. \n","\n","Note that you are required to follow the good practices when training your network. In particular, you need to look at the loss curves (training and validation losses)."]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using CPU\n"]}],"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print('Using GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device('cpu')\n","    print('Using CPU')"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[50], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m# Define the trainer object and train the model\u001b[39;00m\n\u001b[0;32m     73\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model, train_loader, valid_loader, criterion, optimizer, device)\n\u001b[1;32m---> 74\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(num_epochs)\n","Cell \u001b[1;32mIn[50], line 33\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, num_epochs)\u001b[0m\n\u001b[0;32m     31\u001b[0m valid_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader):\n\u001b[0;32m     34\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     35\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n","File \u001b[1;32mc:\\Users\\Hecatia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\Hecatia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\Hecatia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\Hecatia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\Hecatia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n","File \u001b[1;32mc:\\Users\\Hecatia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n","File \u001b[1;32mc:\\Users\\Hecatia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:246\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[0;32m    245\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[0;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","\n","# Define the ResNet model\n","class ResNetModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNetModel, self).__init__()\n","        self.resnet = models.resnet18(pretrained=True)\n","        num_features = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Linear(num_features, num_classes)\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        return x\n","\n","# Define the training class\n","class Trainer:\n","    def __init__(self, model, train_loader, valid_loader, criterion, optimizer, device):\n","        self.model = model\n","        self.train_loader = train_loader\n","        self.valid_loader = valid_loader\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.device = device\n","\n","    def train(self, num_epochs):\n","        self.model.to(self.device)\n","        for epoch in range(num_epochs):\n","            train_loss = 0.0\n","            valid_loss = 0.0\n","            self.model.train()\n","            for i, (inputs, labels) in enumerate(self.train_loader):\n","                inputs = inputs.to(self.device)\n","                labels = labels.to(self.device)\n","                self.optimizer.zero_grad()\n","                outputs = self.model(inputs)\n","                loss = self.criterion(outputs, labels)\n","                loss.backward()\n","                self.optimizer.step()\n","                train_loss += loss.item() * inputs.size(0)\n","            train_loss /= len(self.train_loader.dataset)\n","            self.model.eval()\n","            with torch.no_grad():\n","                for i, (inputs, labels) in enumerate(self.valid_loader):\n","                    inputs = inputs.to(self.device)\n","                    labels = labels.to(self.device)\n","                    outputs = self.model(inputs)\n","                    loss = self.criterion(outputs, labels)\n","                    valid_loss += loss.item() * inputs.size(0)\n","                valid_loss /= len(self.valid_loader.dataset)\n","            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, train_loss, valid_loss))\n","\n","# Define the hyperparameters\n","num_classes = 120\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","# Define the device to use for training\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the ResNet model and move it to the device\n","model = ResNetModel(num_classes).to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Define the device to use for training\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# Define the trainer object and train the model\n","trainer = Trainer(model, train_loader, valid_loader, criterion, optimizer, device)\n","trainer.train(num_epochs)"]},{"cell_type":"markdown","metadata":{"id":"mMQsNkhZ-qTw"},"source":["## **6. Run on the Testing Set and Submit teh Results on Kaggle**\n","\n","Finally, test your trained model on the test set and upload the results to the [Kaggle competition](https://www.kaggle.com/c/dog-breed-identification). \n","\n","You are required to submit a screenshot of your score."]},{"cell_type":"markdown","metadata":{"id":"uCDNmqiL-zqf"},"source":["## **7. Hints to Improve Your Results**\n","- Try to increase the batch size and the number of epochs.\n","- Try deeper ResNet networks.\n"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
